[train] #train parameters
epoch = 16
batch_size = 4

shuffle = True

reader_num = 8

optimizer = AdamW
learning_rate = 1e-4
weight_decay = 0.01
step_size = 1
lr_multiplier = 1

max_len=4096
mlm_prob=0.15

warmup_steps=10000
training_steps=1000000
max_grad_norm=1.0
fp16=True

valid_mode = step
step_epoch = 3000

grad_accumulate = 1

[eval] #eval parameters
batch_size = 12

shuffle = False

reader_num = 4

[distributed]
use = True
backend = nccl

[data] #data parameters
train_dataset_type = MultiDocDataset
train_formatter_type = VanillaLFM
train_data = /mnt/datadisk0/xcj/LegalBert/data/tokens
train_files = ms_data_law_train_SS_document,xs_data_law_train_SS_document

valid_dataset_type = MultiDocDataset
valid_formatter_type = VanillaLFM
valid_data = /mnt/datadisk0/xcj/LegalBert/data/tokens
valid_files = ms_data_law_valid_SS_document,xs_data_law_valid_SS_document

[model] #model parameters
model_name = VanillaLFM

[output] #output parameters
output_time = 1
test_time = 1

model_path = /mnt/datadisk0/xcj/LegalBert/model
model_name = VanillaLFM_AllData4096_lr1e-4

tensorboard_path = /mnt/datadisk0/xcj/LegalBert/tensorboard

output_function = Null
